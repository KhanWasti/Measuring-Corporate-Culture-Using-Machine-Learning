{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4980c0-a8f0-410c-8eb0-284d550a958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp /Users/mai/stanford-corenlp-full-2018-10-05//* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 12000000 -threads 2 -maxCharLength 1000000 -quiet True -serverProperties corenlp_server-a884e4989c44486f.props -preload tokenize, ssplit, pos, lemma, ner, depparse\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20756/36823575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     with CoreNLPClient(\n\u001b[0m\u001b[0;32m     89\u001b[0m         properties={\n\u001b[0;32m     90\u001b[0m             \u001b[1;34m\"ner.applyFineGrained\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"false\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\stanfordnlp\\server\\client.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\stanfordnlp\\server\\client.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Starting server with command: {' '.join(self.start_cmd)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             self.server = subprocess.Popen(self.start_cmd,\n\u001b[0m\u001b[0;32m     96\u001b[0m                                            \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                                            stdout=stderr)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation of parse.py that supports multiprocess\n",
    "Main differences are 1) using Pool.starmap in process_largefile and 2) attach to local CoreNLP server in process_largefile.process_document\n",
    "\"\"\"\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "import global_options\n",
    "from culture import file_util, preprocess_parallel\n",
    "\n",
    "\n",
    "def process_largefile(\n",
    "    input_file,\n",
    "    output_file,\n",
    "    input_file_ids,\n",
    "    output_index_file,\n",
    "    function_name,\n",
    "    chunk_size=100,\n",
    "    start_index=None,\n",
    "):\n",
    "    \"\"\" A helper function that transforms an input file + a list of IDs of each line (documents + document_IDs) to two output files (processed documents + processed document IDs) by calling function_name on chunks of the input files. Each document can be decomposed into multiple processed documents (e.g. sentences). \n",
    "    Supports parallel with Pool.\n",
    "\n",
    "    Arguments:\n",
    "        input_file {str or Path} -- path to a text file, each line is a document\n",
    "        ouput_file {str or Path} -- processed linesentence file (remove if exists)\n",
    "        input_file_ids {str]} -- a list of input line ids\n",
    "        output_index_file {str or Path} -- path to the index file of the output\n",
    "        function_name {callable} -- A function that processes a list of strings, list of ids and return a list of processed strings and ids.\n",
    "        chunk_size {int} -- number of lines to process each time, increasing the default may increase performance\n",
    "        start_index {int} -- line number to start from (index starts with 0)\n",
    "\n",
    "    Writes:\n",
    "        Write the ouput_file and output_index_file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if start_index is None:\n",
    "            # if start from the first line, remove existing output file\n",
    "            # else append to existing output file\n",
    "            os.remove(str(output_file))\n",
    "            os.remove(str(output_index_file))\n",
    "    except OSError:\n",
    "        pass\n",
    "    assert file_util.line_counter(input_file) == len(\n",
    "        input_file_ids\n",
    "    ), \"Make sure the input file has the same number of rows as the input ID file. \"\n",
    "\n",
    "    with open(input_file, newline=\"\\n\", encoding=\"utf-8\", errors=\"ignore\") as f_in:\n",
    "        line_i = 0\n",
    "        # jump to index\n",
    "        if start_index is not None:\n",
    "            # start at start_index line\n",
    "            for _ in range(start_index):\n",
    "                next(f_in)\n",
    "            input_file_ids = input_file_ids[start_index:]\n",
    "            line_i = start_index\n",
    "        for next_n_lines, next_n_line_ids in zip(\n",
    "            itertools.zip_longest(*[f_in] * chunk_size),\n",
    "            itertools.zip_longest(*[iter(input_file_ids)] * chunk_size),\n",
    "        ):\n",
    "            line_i += chunk_size\n",
    "            print(datetime.datetime.now())\n",
    "            print(f\"Processing line: {line_i}.\")\n",
    "            next_n_lines = list(filter(None.__ne__, next_n_lines))\n",
    "            next_n_line_ids = list(filter(None.__ne__, next_n_line_ids))\n",
    "            output_lines = []\n",
    "            output_line_ids = []\n",
    "            with Pool(global_options.N_CORES) as pool:\n",
    "                for output_line, output_line_id in pool.starmap(\n",
    "                    function_name, zip(next_n_lines, next_n_line_ids)\n",
    "                ):\n",
    "                    output_lines.append(output_line)\n",
    "                    output_line_ids.append(output_line_id)\n",
    "            output_lines = \"\\n\".join(output_lines) + \"\\n\"\n",
    "            output_line_ids = \"\\n\".join(output_line_ids) + \"\\n\"\n",
    "            with open(output_file, \"a\", newline=\"\\n\") as f_out:\n",
    "                f_out.write(output_lines)\n",
    "            if output_index_file is not None:\n",
    "                with open(output_index_file, \"a\", newline=\"\\n\") as f_out:\n",
    "                    f_out.write(output_line_ids)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with CoreNLPClient(\n",
    "        properties={\n",
    "            \"ner.applyFineGrained\": \"false\",\n",
    "            \"annotators\": \"tokenize, ssplit, pos, lemma, ner, depparse\",\n",
    "        },\n",
    "        memory=global_options.RAM_CORENLP,\n",
    "        threads=global_options.N_CORES,\n",
    "        timeout=12000000,\n",
    "        endpoint=\"http://localhost:9002\",  # change port here and in preprocess_parallel.py if 9002 is occupied\n",
    "        max_char_length=1000000,\n",
    "    ) as client:\n",
    "        in_file = Path(global_options.DATA_FOLDER, \"input\", \"documents.txt\")\n",
    "        in_file_index = file_util.file_to_list(\n",
    "            Path(global_options.DATA_FOLDER, \"input\", \"document_ids.txt\")\n",
    "        )\n",
    "        out_file = Path(\n",
    "            global_options.DATA_FOLDER, \"processed\", \"parsed\", \"documents.txt\"\n",
    "        )\n",
    "        output_index_file = Path(\n",
    "            global_options.DATA_FOLDER, \"processed\", \"parsed\", \"document_sent_ids.txt\"\n",
    "        )\n",
    "        process_largefile(\n",
    "            input_file=in_file,\n",
    "            output_file=out_file,\n",
    "            input_file_ids=in_file_index,\n",
    "            output_index_file=output_index_file,\n",
    "            function_name=preprocess_parallel.process_document,\n",
    "            chunk_size=global_options.PARSE_CHUNK_SIZE,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
