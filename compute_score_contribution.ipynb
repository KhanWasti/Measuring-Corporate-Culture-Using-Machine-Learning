{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4980c0-a8f0-410c-8eb0-284d550a958f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\w2v\\\\w2v.mod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mculture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m culture_dictionary\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw2v\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw2v.mod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m vocab_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocab size in the w2v model: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vocab_number))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[1;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \n\u001b[0;32m   1940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1942\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Word2Vec, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[0;32m   1944\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models\\\\w2v\\\\w2v.mod'"
     ]
    }
   ],
   "source": [
    "# Compute contribution of each words (for TF, TFIDF, WFIDF)\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import global_options\n",
    "import score\n",
    "from culture import culture_dictionary\n",
    "\n",
    "\n",
    "def recalculate_contribution(\n",
    "    documents, document_ids, all_dict_words, df_dict, N_doc, word_weights=None\n",
    "):\n",
    "    contribution_TF = defaultdict(int)\n",
    "    contribution_WFIDF = defaultdict(int)\n",
    "    contribution_TFIDF = defaultdict(int)\n",
    "    contribution_TFIDF_SIMWEIGHT = defaultdict(int)\n",
    "    contribution_WFIDF_SIMWEIGHT = defaultdict(int)\n",
    "    for i, doc in enumerate(tqdm(documents)):\n",
    "        document = doc.split()\n",
    "        c = Counter(document)\n",
    "        for pair in c.items():\n",
    "            if pair[0] in all_dict_words:\n",
    "                contribution_TF[pair[0]] += pair[1]\n",
    "                w_ij = (1 + math.log(pair[1])) * math.log(N_doc / df_dict[pair[0]])\n",
    "                contribution_WFIDF[pair[0]] += w_ij\n",
    "                w_ij = pair[1] * math.log(N_doc / df_dict[pair[0]])\n",
    "                contribution_TFIDF[pair[0]] += w_ij\n",
    "                w_ij = (\n",
    "                    pair[1] * word_weights[pair[0]] * math.log(N_doc / df_dict[pair[0]])\n",
    "                )\n",
    "                contribution_TFIDF_SIMWEIGHT[pair[0]] += w_ij\n",
    "                w_ij = (\n",
    "                    (1 + math.log(pair[1]))\n",
    "                    * word_weights[pair[0]]\n",
    "                    * math.log(N_doc / df_dict[pair[0]])\n",
    "                )\n",
    "                contribution_WFIDF_SIMWEIGHT[pair[0]] += w_ij\n",
    "    contribution_dict = {\n",
    "        \"TF\": contribution_TF,\n",
    "        \"TFIDF\": contribution_TFIDF,\n",
    "        \"WFIDF\": contribution_WFIDF,\n",
    "        \"TFIDF+SIMWEIGHT\": contribution_TFIDF_SIMWEIGHT,\n",
    "        \"WFIDF+SIMWEIGHT\": contribution_WFIDF_SIMWEIGHT,\n",
    "    }\n",
    "    return contribution_dict\n",
    "\n",
    "\n",
    "def output_contribution(contribution_dict, out_file):\n",
    "    \"\"\"output contribution dict to Excel file\n",
    "    Arguments:\n",
    "        contribution_dict {word:contribution} -- a pre-calculated contribution dict for each word in expanded dictionary\n",
    "        out_file {str} -- file name (Excel)\n",
    "    \"\"\"\n",
    "    contribution_lst = []\n",
    "    for dim in culture_dict:\n",
    "        for w in culture_dict[dim]:\n",
    "            w_dict = {}\n",
    "            w_dict[\"dim\"] = dim\n",
    "            w_dict[\"word\"] = w\n",
    "            w_dict[\"contribution\"] = contribution_dict[w]\n",
    "            contribution_lst.append(w_dict)\n",
    "\n",
    "    contribution_df = pd.DataFrame(contribution_lst)\n",
    "    dim_dfs = []\n",
    "    for dim in sorted(culture_dict.keys()):\n",
    "        dim_df = (\n",
    "            contribution_df[contribution_df.dim == dim]\n",
    "            .sort_values(by=\"contribution\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        dim_df[\"total_contribution\"] = dim_df[\"contribution\"].sum()\n",
    "        dim_df[\"relative_contribuion\"] = dim_df[\"contribution\"].div(\n",
    "            dim_df[\"total_contribution\"]\n",
    "        )\n",
    "        dim_df[\"cumulative_contribution\"] = dim_df[\"relative_contribuion\"].cumsum()\n",
    "        dim_df = dim_df.drop([\"total_contribution\"], axis=1)\n",
    "        dim_dfs.append(dim_df)\n",
    "    pd.concat(dim_dfs, axis=1).to_csv(out_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_dict_path = str(\n",
    "        str(Path(global_options.OUTPUT_FOLDER, \"dict\", \"expanded_dict.csv\"))\n",
    "    )\n",
    "    culture_dict, all_dict_words = culture_dictionary.read_dict_from_csv(\n",
    "        current_dict_path\n",
    "    )\n",
    "\n",
    "    word_sim_weights = culture_dictionary.compute_word_sim_weights(current_dict_path)\n",
    "    corpus, doc_ids, N_doc = score.load_doc_level_corpus()\n",
    "    df_dict = pd.read_pickle(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"doc_freq.pickle\")\n",
    "    )\n",
    "    contributions_final_sample = recalculate_contribution(\n",
    "        documents=corpus,\n",
    "        document_ids=doc_ids,\n",
    "        all_dict_words=all_dict_words,\n",
    "        df_dict=df_dict,\n",
    "        N_doc=N_doc,\n",
    "        word_weights=word_sim_weights,\n",
    "    )\n",
    "    with open(\n",
    "        Path(\n",
    "            global_options.OUTPUT_FOLDER,\n",
    "            \"scores\",\n",
    "            \"word_contributions\",\n",
    "            \"contribution_final_sample.pickle\",\n",
    "        ),\n",
    "        \"wb\",\n",
    "    ) as f:\n",
    "        pickle.dump(contributions_final_sample, f)\n",
    "    for weight_method in [\"TF\", \"TFIDF\", \"WFIDF\"]:\n",
    "        output_contribution(\n",
    "            contributions_final_sample[f\"{weight_method}\"],\n",
    "            Path(\n",
    "                global_options.OUTPUT_FOLDER,\n",
    "                \"scores\",\n",
    "                \"word_contributions\",\n",
    "                f\"word_contribution_{weight_method}.csv\",\n",
    "            ),\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
