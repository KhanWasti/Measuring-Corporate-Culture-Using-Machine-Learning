{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4980c0-a8f0-410c-8eb0-284d550a958f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\w2v\\\\w2v.mod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mculture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m culture_dictionary\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw2v\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw2v.mod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m vocab_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocab size in the w2v model: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vocab_number))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[1;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \n\u001b[0;32m   1940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1942\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Word2Vec, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[0;32m   1944\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1460\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models\\\\w2v\\\\w2v.mod'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import global_options\n",
    "from culture import culture_dictionary, file_util\n",
    "\n",
    "# @TODO: The scoring functions are not memory friendly. The entire pocessed corpus needs to fit in the RAM. Rewrite a memory friendly version.\n",
    "\n",
    "\n",
    "def construct_doc_level_corpus(sent_corpus_file, sent_id_file):\n",
    "    \"\"\"Construct document level corpus from sentence level corpus and write to disk.\n",
    "    Dump \"corpus_doc_level.pickle\" and \"doc_ids.pickle\" to Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\"). \n",
    "    \n",
    "    Arguments:\n",
    "        sent_corpus_file {str or Path} -- The sentence corpus after parsing and cleaning, each line is a sentence\n",
    "        sent_id_file {str or Path} -- The sentence ID file, each line correspond to a line in the sent_co(docID_sentenceID)\n",
    "    \n",
    "    Returns:\n",
    "        [str], [str], int -- a tuple of a list of documents, a list of document IDs, and the number of documents\n",
    "    \"\"\"\n",
    "    print(\"Constructing doc level corpus\")\n",
    "    # sentence level corpus\n",
    "    sent_corpus = file_util.file_to_list(sent_corpus_file)\n",
    "    sent_IDs = file_util.file_to_list(sent_id_file)\n",
    "    assert len(sent_IDs) == len(sent_corpus)\n",
    "    # doc id for each sentence\n",
    "    doc_ids = [x.split(\"_\")[0] for x in sent_IDs]\n",
    "    # concat all text from the same doc\n",
    "    id_doc_dict = defaultdict(lambda: \"\")\n",
    "    for i, id in enumerate(doc_ids):\n",
    "        id_doc_dict[id] += \" \" + sent_corpus[i]\n",
    "    # create doc level corpus\n",
    "    corpus = list(id_doc_dict.values())\n",
    "    doc_ids = list(id_doc_dict.keys())\n",
    "    assert len(corpus) == len(doc_ids)\n",
    "    with open(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"corpus_doc_level.pickle\"),\n",
    "        \"wb\",\n",
    "    ) as out_f:\n",
    "        pickle.dump(corpus, out_f)\n",
    "    with open(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"doc_ids.pickle\"), \"wb\"\n",
    "    ) as out_f:\n",
    "        pickle.dump(doc_ids, out_f)\n",
    "    N_doc = len(corpus)\n",
    "    return corpus, doc_ids, N_doc\n",
    "\n",
    "\n",
    "def calculate_df(corpus):\n",
    "    \"\"\"Calcualte and dump a document-freq dict for all the words.\n",
    "    \n",
    "    Arguments:\n",
    "        corpus {[str]} -- a list of documents\n",
    "    \n",
    "    Returns:\n",
    "        {dict[str: int]} -- document freq for each word\n",
    "    \"\"\"\n",
    "    print(\"Calculating document frequencies.\")\n",
    "    # document frequency\n",
    "    df_dict = defaultdict(int)\n",
    "    for doc in tqdm(corpus):\n",
    "        doc_splited = doc.split()\n",
    "        words_in_doc = set(doc_splited)\n",
    "        for word in words_in_doc:\n",
    "            df_dict[word] += 1\n",
    "    # save df dict\n",
    "    with open(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"doc_freq.pickle\"), \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(df_dict, f)\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def load_doc_level_corpus():\n",
    "    \"\"\"load the corpus constructed by construct_doc_level_corpus()\n",
    "    \n",
    "    Returns:\n",
    "        [str], [str], int -- a tuple of a list of documents, a list of document IDs, and the number of documents\n",
    "    \"\"\"\n",
    "    print(\"Loading document level corpus.\")\n",
    "    with open(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"corpus_doc_level.pickle\"),\n",
    "        \"rb\",\n",
    "    ) as in_f:\n",
    "        corpus = pickle.load(in_f)\n",
    "    with open(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"doc_ids.pickle\"), \"rb\"\n",
    "    ) as in_f:\n",
    "        doc_ids = pickle.load(in_f)\n",
    "    assert len(corpus) == len(doc_ids)\n",
    "    N_doc = len(corpus)\n",
    "    return corpus, doc_ids, N_doc\n",
    "\n",
    "\n",
    "def score_tf(documents, doc_ids, expanded_dict):\n",
    "    \"\"\"\n",
    "    Score documents using term freq. \n",
    "    \"\"\"\n",
    "    print(\"Scoring using Term-freq (tf).\")\n",
    "    score = culture_dictionary.score_tf(\n",
    "        documents=documents,\n",
    "        document_ids=doc_ids,\n",
    "        expanded_words=expanded_dict,\n",
    "        n_core=global_options.N_CORES,\n",
    "    )\n",
    "    score.to_csv(\n",
    "        Path(global_options.OUTPUT_FOLDER, \"scores\", \"scores_TF.csv\"), index=False\n",
    "    )\n",
    "\n",
    "\n",
    "def score_tf_idf(documents, doc_ids, N_doc, method, expanded_dict, **kwargs):\n",
    "    \"\"\"Score documents using tf-idf and its variations\n",
    "    \n",
    "    Arguments:\n",
    "        documents {[str]} -- list of documents\n",
    "        doc_ids {[str]} -- list of document IDs\n",
    "        N_doc {int} -- number of documents\n",
    "        method {str} -- \n",
    "            TFIDF: conventional tf-idf \n",
    "            WFIDF: use wf-idf log(1+count) instead of tf in the numerator\n",
    "            TFIDF/WFIDF+SIMWEIGHT: using additional word weights given by the word_weights dict\n",
    "        expanded_dict {dict[str, set(str)]} -- expanded dictionary\n",
    "    \"\"\"\n",
    "    if method == \"TF\":\n",
    "        print(\"Scoring TF.\")\n",
    "        score_tf(documents, doc_ids, expanded_dict)\n",
    "    else:\n",
    "        print(\"Scoring TF-IDF.\")\n",
    "        # load document freq\n",
    "        df_dict = pd.read_pickle(\n",
    "            Path(global_options.OUTPUT_FOLDER, \"scores\", \"temp\", \"doc_freq.pickle\")\n",
    "        )\n",
    "        # score tf-idf\n",
    "        score, contribution = culture_dictionary.score_tf_idf(\n",
    "            documents=documents,\n",
    "            document_ids=doc_ids,\n",
    "            expanded_words=expanded_dict,\n",
    "            df_dict=df_dict,\n",
    "            N_doc=N_doc,\n",
    "            method=method,\n",
    "            **kwargs\n",
    "        )\n",
    "        # save the document level scores (without dividing by doc length)\n",
    "        score.to_csv(\n",
    "            str(\n",
    "                Path(\n",
    "                    global_options.OUTPUT_FOLDER,\n",
    "                    \"scores\",\n",
    "                    \"scores_{}.csv\".format(method),\n",
    "                )\n",
    "            ),\n",
    "            index=False,\n",
    "        )\n",
    "        # save word contributions\n",
    "        pd.DataFrame.from_dict(contribution, orient=\"index\").to_csv(\n",
    "            Path(\n",
    "                global_options.OUTPUT_FOLDER,\n",
    "                \"scores\",\n",
    "                \"word_contributions\",\n",
    "                \"word_contribution_{}.csv\".format(method),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_dict_path = str(\n",
    "        str(Path(global_options.OUTPUT_FOLDER, \"dict\", \"expanded_dict.csv\"))\n",
    "    )\n",
    "    culture_dict, all_dict_words = culture_dictionary.read_dict_from_csv(\n",
    "        current_dict_path\n",
    "    )\n",
    "    # words weighted by similarity rank (optional)\n",
    "    word_sim_weights = culture_dictionary.compute_word_sim_weights(current_dict_path)\n",
    "\n",
    "    ## Pre-score ===========================\n",
    "    # aggregate processed sentences to documents\n",
    "    corpus, doc_ids, N_doc = construct_doc_level_corpus(\n",
    "        sent_corpus_file=Path(\n",
    "            global_options.DATA_FOLDER, \"processed\", \"trigram\", \"documents.txt\"\n",
    "        ),\n",
    "        sent_id_file=Path(\n",
    "            global_options.DATA_FOLDER, \"processed\", \"parsed\", \"document_sent_ids.txt\"\n",
    "        ),\n",
    "    )\n",
    "    word_doc_freq = calculate_df(corpus)\n",
    "\n",
    "    ## Score ========================\n",
    "    # create document scores\n",
    "    methods = [\"TF\", \"TFIDF\", \"WFIDF\"]\n",
    "    for method in methods:\n",
    "        score_tf_idf(\n",
    "            corpus,\n",
    "            doc_ids,\n",
    "            N_doc,\n",
    "            method=method,\n",
    "            expanded_dict=culture_dict,\n",
    "            normalize=False,\n",
    "            word_weights=word_sim_weights,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
